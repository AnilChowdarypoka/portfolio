[
  {
    "company": "The Walt Disney Company",
    "role": "Senior Software Engineer",
    "period": "09/2024- Present",
    "location": "Glendale, California",
    "achievements": [
      "Created and constructed intricate Spark (Scala) ETL pipelines to facilitate end-to-end workflows for Hulu Ads measurement and vendor attribution, providing standardized and reliable content delivery.",
      "Engineered and orchestrated Airflow DAGs using AWS MWAA, creating, and managing 10+ Spark ETL pipelines, processing over 50M ad log events daily, ensuring scalable and reliable execution.",
      "Converted Spark batch pipelines into streaming workflows via Spark Streaming, reducing ingestion latency from ~2 hours to 20 minutes and enabling near real-time content delivery for ad measurement.",
      "Facilitated the migration and replication of Business Intelligence team pipelines, implementing custom business logics, validations, and error handling to meet Hulu Ads platform and downstream dependencies.",
      "Built and maintained data quality pipelines to validate critical datasets, improving accuracy and completeness by ~15% across multiple ETL workflows.",
      "Engaged in Databricks migration proof of concepts (POCs) for assessing Spark workloads in terms of performance, scaling opportunities, and integration with existing cloud-based ETL architecture.",
      "Supported attribution and vendor-facing pipelines, engineering secure and customizable workflows that allowed internal and external teams to easily access ad logs and campaign metrics.",
      "Created SQL-based transformations and Snowflake external stage pipelines to include key-based authentication in lieu of password-based authentication, strengthening security, compliance, and maintainability.",
      "Helped with data ingestion, transformation, and validation workflows in AWS Glue, S3, and Athena to produce standardized, quality data across various pipelines.",
      "Assisted with deployment and orchestration of Spark jobs on EKS with Spinnaker for greater reliability, reproducibility, and improved operations over multiple pipelines.",
      "Monitored health of pipelines and handled incidents using PagerDuty, collaborated through JIRA and Lucidchart, and documented all workflows, migrations, and process improvements.",
      "Worked collaboratively with cross-functional teams in Agile Scrum sprints, coordinating with product managers, analysts, and stakeholders to prioritize tasks, clarify requirements, and ensure timely delivery of data solutions."
    ]
  },
  {
    "company": "Verizon",
    "role": "Senior Data Engineer",
    "period": "08/2024- 09/2025",
    "location": "Basking Ridge, New Jersey",
    "achievements": [
      "Created, configured, and launched end-to-end data pipelines utilizing Amazon S3, Redshift, AWS Glue, and PySpark to process large volumes of structured and unstructured telecom, billing, and customer data.",
      "Developed and sustained horizontally scalable big data architectures on Amazon EMR (Hadoop and Spark) supporting batch and real-time analytics for mission-critical workloads.",
      "Engineered real-time data ingestion pipelines utilizing Kafka, Amazon Kinesis, and Apache Flink to reduce fraud alert latency and support event-driven and batch processing.",
      "Developed metadata driven ETL pipelines to streamline onboarding of new data sources and reduce deployment cycles.",
      "Designed and implemented a Medallion Architecture-based data lake on Amazon S3 using bronze, silver, and gold layers to enable scalable, governed, and modular data pipelines.",
      "Optimized Redshift and DynamoDB performance using advanced SQL, partitioning, and indexing for faster, efficient queries.",
      "Designed anomaly detection workflows on AWS using Glue, SageMaker, and Python, based on statistical thresholds and time-series patterns.",
      "Built automation scripts in Python/Shell on Linux-based EC2 instances for data ingestion, transformation, and reporting.",
      "Orchestrated Spark and ETL workflows via Airflow integrating Redshift, AWS Glue, and S3, improving reliability by 35%.",
      "Developed DBT models to transform raw data into curated datasets for analytics and reporting across AWS Redshift and BigQuery environments.",
      "Automated data quality checks and audit logging for trustworthy, traceable, and compliant data ingestion pipelines.",
      "Implemented monitoring and alerting via CloudWatch and SNS to enforce SLA compliance and support incident response.",
      "Built and deployed secure RESTful APIs using API Gateway and Lambda for real-time access to curated datasets across teams.",
      "Enforced fine-grained access control with AWS Lake Formation, IAM roles, and KMS encryption to ensure security and compliance.",
      "Built interactive dashboards in Tableau and Power BI to provide real-time visibility into KPIs and operational metrics.",
      "Implemented Jenkins-based CI/CD pipelines using AWS CodePipeline, CodeBuild, and GitHub Actions, ensuring reliable change management and deployment practices.",
      "Managed Agile project milestones using Scrum methodology; conducted RCA and improved system reliability by 30%."
    ]
  },
  {
    "company": "Verisk Underwriting",
    "role": "Data Engineer",
    "period": "06/2023- 08/2024",
    "location": "Jersey City, New Jersey",
    "achievements": [
      "Architected and optimized large-scale Apache Spark pipelines on AWS EMR, processing 5TB+ of trades and market data daily, enabling a 60% faster performance of risk analytics and regulatory reporting.",
      "Developed and maintained real-time streaming ingestion pipelines utilizing Kafka, Apache Flink, and AWS Lambda, cutting fraud detection latency from hours to less than five minutes to allow for faster risk mitigation.",
      "Migrated legacy batch ETL workflows to distributed Spark jobs on Hadoop YARN, bringing forth 40% enhanced performance of jobs and into a higher mode of scaling with increasing data volumes.",
      "Designed and optimized Hive and Impala data warehouses with partitioning, bucketing, and indexing leading to a 35% improvement in query performance.",
      "Diagnosed and remediated Spark and SQL performance-related workflow production incidents using Splunk and Cloudera tools, which resulted in an average of 30% improvement in time to resolve.",
      "Developed and optimized complex ETL pipelines in Informatica PowerCenter, enhancing data accuracy and processing efficiency for critical financial reporting.",
      "Built proactive Splunk alerts and dashboards to monitor ETL workflows and infrastructure, cutting response time by 30%.",
      "Created Python-based pipelines to clean, validate, and transform data, enhancing data quality and reducing anomalies by 25%.",
      "Automated ETL orchestration using Apache Airflow and Oozie, executing dependencies of Spark, Hive, Impala, and HBase jobs concurrently, maintaining 99.9% pipeline reliability and eliminating manual effort.",
      "Created schema-aware ingestion frameworks for structured and semi-structured sources with Apache Avro, Parquet, and Delta Lake, enabling interoperability across various analytics teams.",
      "Developed containerized microservices using Docker and Kubernetes for superior portability and faster application deployment.",
      "Maintained and optimized large-scale datasets in HDFS, implementing data lifecycle policies and supporting high-throughput batch ETL jobs for credit risk analytics."
    ]
  },
  {
    "company": "Digit Insurance",
    "role": "Associate Data Engineer",
    "period": "07/2021- 12/2022",
    "location": "Bangalore, India",
    "achievements": [
      "Engineered and scaled data solutions using AWS services and big data technologies to enhance processing capabilities and support evolving business objectives, reducing operational bottlenecks by 20 hours monthly.",
      "Worked collaboratively and cross-functionally to design validated data models and deploy SOAP/REST APIs, increasing data access and improving real-time updates in interoperability model.",
      "Consolidated unstructured API logs from over 500 clients by designing a scalable data pipeline with AWS DMS, Kafka, and AWS Lambda, increasing visibility around failure detection in real-time and lowering average system outages by 40%.",
      "Implemented a scalable real-time telematics data processing pipeline using AWS, Kafka, and PySpark that analyzes driving behavior and travel patterns, enabling the pricing of insurance premiums in a personalized manner to improve pricing accuracy and customer satisfaction.",
      "Architected and implemented batch processing architecture utilizing Airflow with reconciliation logic to process delay insurance data from multiple vendors, increasing claims accuracy and reducing time to resolution by 30%.",
      "Led the development of a KYC module for life insurance utilizing Python, Azure, and MongoDB, connecting multiple clients and creating a secured data lake, cutting down data verification time significantly.",
      "Automated ETL data workflows using Informatica and implemented Splunk-based monitoring to ensure data pipeline reliability and faster issue resolution.",
      "Developed advanced SQL procedures and optimized complex queries in PostgreSQL and MySQL for insurance billing and usage analytics, reducing query execution time by 40% and supporting faster business reporting.",
      "Automated real-time analytics and notifications to manage insurance claim pre-inspection using Apache Kafka, Flink, Airflow, PostgreSQL, and AWS Lambda, improving reliability and reducing downtime.",
      "Designed a centralized data lake on Amazon S3 and Azure Data Lake Storage to efficiently manage structured, semi-structured, and unstructured data while designing scalable ETL pipelines.",
      "Automated the reporting process using Python, SQL, and BI tools like Tableau, Qlik, and Power BI, reducing report time by 20%."
    ]
  },
  {
    "company": "Prospecta Software",
    "role": "Technical Trainee",
    "period": "08/2020- 07/2021",
    "location": "Gurgaon, India",
    "achievements": [
      "Designed custom business logic in MDO Portal using Java, Python, SQL, XML, and Shell scripting for optimal performance.",
      "Implemented event-driven ETL workflows leveraging AWS Lambda and S3 event triggers to automate ingestion of user eligibility and medication adherence data, reducing ingestion latency from hours to minutes.",
      "Built real-time data pipelines on AWS to ingest patient vitals and device readings via AWS IoT Core, enabling real-time health alerts for clinical trials with SQL-based trial monitoring queries.",
      "Designed data integration frameworks that combined transactional PostgreSQL and clinical NoSQL repositories into holistic analytics lakes, ensuring compliance with HL7/FHIR standards.",
      "Created Spark Structured Streaming pipelines with Kafka for near real-time ingestion of pharmacy benefit management (PBM) transaction data, increasing claim adjudication velocity and accuracy by 30%.",
      "Oversaw migration of legacy ETL batch pipelines to cloud-native AWS stacks, reducing infrastructure expenditures by 25% while increasing scale, security, and HIPAA/HITRUST compliance.",
      "Worked with DevOps teams to establish CI/CD pipelines using Jenkins, GitHub Actions, and AWS CodePipeline, reducing deployment times and improving reliability of data pipelines."
    ]
  }
]
